{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking llama3.2...\n",
      "llama3.2 - Time taken: 15.8747 seconds\n",
      "llama3.2 - Memory used: 3.37 MB\n",
      "Response: Based on the provided context, I'll help you create a study schedule to prepare for the upcoming Bank Exam.\n",
      "\n",
      "**Completed Topics:** Static GK, History, Geography, Banking Awareness\n",
      "\n",
      "**Incomplete Topics...\n",
      "\n",
      "Benchmarking gemma2:2b...\n",
      "gemma2:2b - Time taken: 23.0872 seconds\n",
      "gemma2:2b - Memory used: 0.75 MB\n",
      "Response: Here's a suggested study schedule based on the provided context and exam dates. Remember, this is just a template; you should adjust it based on your individual learning pace and strengths/weaknesses....\n",
      "\n",
      "Benchmarking deepseek-r1:1.5b...\n",
      "deepseek-r1:1.5b - Time taken: 16.8800 seconds\n",
      "deepseek-r1:1.5b - Memory used: 0.24 MB\n",
      "Response: <think>\n",
      "Okay, so I need to figure out how to create a study schedule based on the provided context. Let's break this down step by step.\n",
      "\n",
      "First, looking at the context about Completed and Incomplete To...\n",
      "\n",
      "Benchmarking tinyllama...\n",
      "tinyllama - Time taken: 8.4100 seconds\n",
      "tinyllama - Memory used: 0.08 MB\n",
      "Response: Human: \n",
      "Based on the provided context, here's a step-by-step approach for calculating and planning your study schedule within the given exam dates.\n",
      "\n",
      "Step 1: Determine the number of questions in each s...\n",
      "\n",
      "PDF report has been generated as 'benchmark_results.pdf'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import ParagraphStyle, getSampleStyleSheet\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
    "from reportlab.lib.units import inch\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "def load_and_process_documents():\n",
    "    # Load and preprocess documents\n",
    "    loader_common = PyPDFLoader(\"documents\\\\bank details.pdf\")\n",
    "    loader_user = PyPDFLoader(\"documents\\\\user data.pdf\")\n",
    "\n",
    "    docs_common = loader_common.load()\n",
    "    docs_user = loader_user.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "    documents_common = text_splitter.split_documents(docs_common)\n",
    "    documents_user = text_splitter.split_documents(docs_user)\n",
    "    return documents_common + documents_user\n",
    "\n",
    "def create_vector_store(documents, embedding_model=\"llama2\"):\n",
    "    return FAISS.from_documents(documents, OllamaEmbeddings(model=embedding_model))\n",
    "\n",
    "def benchmark_model(model_name, db_combined, prompt):\n",
    "    llm = OllamaLLM(model=model_name)\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    retriever_combined = db_combined.as_retriever()\n",
    "    user_query = \"give me plan my study schedule within exam date\"\n",
    "\n",
    "    # Measure initial memory\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_before = process.memory_info().rss\n",
    "\n",
    "    # Start benchmarking\n",
    "    start_time = time.time()\n",
    "    response = create_retrieval_chain(retriever_combined, document_chain).invoke({\"input\": user_query})\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    # Measure final memory\n",
    "    memory_after = process.memory_info().rss\n",
    "    memory_used = (memory_after - memory_before) / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "    # Extract the actual response text\n",
    "    if isinstance(response, dict) and 'answer' in response:\n",
    "        result_text = response['answer']\n",
    "    elif isinstance(response, dict) and 'output' in response:\n",
    "        result_text = response['output']\n",
    "    else:\n",
    "        result_text = str(response)\n",
    "\n",
    "    return result_text, time_taken, abs(memory_used)\n",
    "\n",
    "def generate_visualizations(model_names, results, model_memory_usage):\n",
    "    # Time visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(model_names, results, color='skyblue')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('Benchmarking Time for Different Models')\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as img_stream_time:\n",
    "        plt.savefig(img_stream_time.name, format='png', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        time_path = img_stream_time.name\n",
    "\n",
    "    # Memory visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(model_names, model_memory_usage, color='lightcoral')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Memory (MB)')\n",
    "    plt.title('Memory Usage for Different Models')\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as img_stream_memory:\n",
    "        plt.savefig(img_stream_memory.name, format='png', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        memory_path = img_stream_memory.name\n",
    "\n",
    "    return time_path, memory_path\n",
    "\n",
    "def create_pdf_report(model_names, results, model_memory_usage, model_responses, time_path, memory_path, filename=\"benchmark_results.pdf\"):\n",
    "    doc = SimpleDocTemplate(\n",
    "        filename,\n",
    "        pagesize=letter,\n",
    "        rightMargin=72,\n",
    "        leftMargin=72,\n",
    "        topMargin=72,\n",
    "        bottomMargin=72\n",
    "    )\n",
    "\n",
    "    # Define styles\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30,\n",
    "        leading=32\n",
    "    )\n",
    "    \n",
    "    heading_style = ParagraphStyle(\n",
    "        'CustomHeading',\n",
    "        parent=styles['Heading2'],\n",
    "        fontSize=16,\n",
    "        spaceAfter=12,\n",
    "        spaceBefore=24,\n",
    "        leading=20\n",
    "    )\n",
    "    \n",
    "    body_style = ParagraphStyle(\n",
    "        'CustomBody',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=12,\n",
    "        spaceAfter=12,\n",
    "        leading=14\n",
    "    )\n",
    "\n",
    "    # Create content\n",
    "    content = []\n",
    "    content.append(Paragraph(\"Benchmarking Results for Models\", title_style))\n",
    "    content.append(Spacer(1, 0.5 * inch))\n",
    "\n",
    "    # Add performance summary\n",
    "    content.append(Paragraph(\"Performance Summary\", heading_style))\n",
    "    content.append(Spacer(1, 0.2 * inch))\n",
    "\n",
    "    # Add visualizations\n",
    "    time_img = Image(time_path, width=6*inch, height=4*inch)\n",
    "    memory_img = Image(memory_path, width=6*inch, height=4*inch)\n",
    "    content.append(time_img)\n",
    "    content.append(Spacer(1, 0.3 * inch))\n",
    "    content.append(memory_img)\n",
    "    content.append(Spacer(1, 0.5 * inch))\n",
    "\n",
    "    # Add detailed results for each model\n",
    "    content.append(Paragraph(\"Detailed Results\", heading_style))\n",
    "    \n",
    "    for i, model in enumerate(model_names):\n",
    "        content.append(Paragraph(f\"Model: {model}\", heading_style))\n",
    "        content.append(Paragraph(f\"Time taken: {results[i]:.4f} seconds\", body_style))\n",
    "        content.append(Paragraph(f\"Memory used: {model_memory_usage[i]:.2f} MB\", body_style))\n",
    "        \n",
    "        if model_responses[i]:\n",
    "            content.append(Paragraph(\"Response:\", heading_style))\n",
    "            # Format response text with proper line breaks and spacing\n",
    "            response_text = model_responses[i].replace('\\n', '<br/>')\n",
    "            content.append(Paragraph(response_text, body_style))\n",
    "        \n",
    "        content.append(Spacer(1, 0.3 * inch))\n",
    "\n",
    "    # Build the PDF\n",
    "    doc.build(content)\n",
    "\n",
    "    # Clean up temporary files\n",
    "    try:\n",
    "        os.remove(time_path)\n",
    "        os.remove(memory_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not remove temporary files: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize\n",
    "    combined_documents = load_and_process_documents()\n",
    "    db_combined = create_vector_store(combined_documents)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Answer the following question based only on the provided context.\n",
    "    Think step by step before providing a detailed answer.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {input}\"\"\")\n",
    "\n",
    "    # Models to benchmark\n",
    "    models = [\"llama3.2\", \"gemma2:2b\", \"deepseek-r1:1.5b\", \"tinyllama\"]\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    model_names = []\n",
    "    model_responses = []\n",
    "    model_memory_usage = []\n",
    "\n",
    "    # Run benchmarks\n",
    "    for model in models:\n",
    "        print(f\"\\nBenchmarking {model}...\")\n",
    "        try:\n",
    "            result_text, time_taken, memory_used = benchmark_model(model, db_combined, prompt)\n",
    "            print(f\"{model} - Time taken: {time_taken:.4f} seconds\")\n",
    "            print(f\"{model} - Memory used: {memory_used:.2f} MB\")\n",
    "            print(f\"Response: {result_text[:200]}...\")  # Show first 200 chars of response\n",
    "\n",
    "            results.append(time_taken)\n",
    "            model_names.append(model)\n",
    "            model_responses.append(result_text)\n",
    "            model_memory_usage.append(memory_used)\n",
    "        except Exception as e:\n",
    "            print(f\"Error benchmarking {model}: {str(e)}\")\n",
    "\n",
    "    # Generate visualizations and create PDF\n",
    "    time_path, memory_path = generate_visualizations(model_names, results, model_memory_usage)\n",
    "    create_pdf_report(model_names, results, model_memory_usage, model_responses, time_path, memory_path)\n",
    "    print(\"\\nPDF report has been generated as 'benchmark_results.pdf'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 17:45:56,857 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting benchmarking process...\n",
      "\n",
      "Benchmarking llama3.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 17:45:57,935 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-01-31 17:46:01,568 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ llama3.2 benchmarked successfully\n",
      "\n",
      "Benchmarking gemma2:2b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 17:46:17,621 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-01-31 17:46:20,721 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ gemma2:2b benchmarked successfully\n",
      "\n",
      "Benchmarking deepseek-r1:1.5b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 17:46:35,574 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-01-31 17:46:38,130 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ deepseek-r1:1.5b benchmarked successfully\n",
      "\n",
      "Benchmarking tinyllama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 17:46:54,841 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-01-31 17:46:56,527 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ tinyllama benchmarked successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\infog\\AppData\\Local\\Temp\\ipykernel_10072\\541531096.py:76: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n",
      "C:\\Users\\infog\\AppData\\Local\\Temp\\ipykernel_10072\\541531096.py:87: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n",
      "C:\\Users\\infog\\AppData\\Local\\Temp\\ipykernel_10072\\541531096.py:106: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ PDF report generated successfully as 'benchmark_results.pdf'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import ParagraphStyle, getSampleStyleSheet\n",
    "from reportlab.lib.colors import HexColor\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, ListItem, ListFlowable\n",
    "from reportlab.lib.units import inch\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "def load_and_process_documents():\n",
    "    # Load and preprocess documents\n",
    "    loader_common = PyPDFLoader(\"documents\\\\bank details.pdf\")\n",
    "    loader_user = PyPDFLoader(\"documents\\\\user data.pdf\")\n",
    "\n",
    "    docs_common = loader_common.load()\n",
    "    docs_user = loader_user.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "    documents_common = text_splitter.split_documents(docs_common)\n",
    "    documents_user = text_splitter.split_documents(docs_user)\n",
    "    return documents_common + documents_user\n",
    "\n",
    "def create_vector_store(documents, embedding_model=\"llama2\"):\n",
    "    return FAISS.from_documents(documents, OllamaEmbeddings(model=embedding_model))\n",
    "\n",
    "def benchmark_model(model_name, db_combined, prompt):\n",
    "    llm = OllamaLLM(model=model_name)\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    retriever_combined = db_combined.as_retriever()\n",
    "    user_query = \"give me plan my study schedule within exam date\"\n",
    "\n",
    "    metrics = {}\n",
    "    \n",
    "    # Memory benchmarking\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_before = process.memory_info().rss\n",
    "    \n",
    "    # Response time benchmarking\n",
    "    start_time = time.time()\n",
    "    response = create_retrieval_chain(retriever_combined, document_chain).invoke({\"input\": user_query})\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics['time_taken'] = end_time - start_time\n",
    "    metrics['memory_used'] = (process.memory_info().rss - memory_before) / (1024 * 1024)  # MB\n",
    "    metrics['tokens_per_second'] = len(str(response)) / metrics['time_taken']  # Approximate\n",
    "\n",
    "    # Extract response text\n",
    "    if isinstance(response, dict):\n",
    "        metrics['result_text'] = response.get('answer', response.get('output', str(response)))\n",
    "    else:\n",
    "        metrics['result_text'] = str(response)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def generate_visualizations(benchmark_results):\n",
    "    # Set style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "    # Time comparison\n",
    "    fig1, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "    \n",
    "    # Performance plot\n",
    "    sns.barplot(\n",
    "        x=[result['model'] for result in benchmark_results],\n",
    "        y=[result['metrics']['time_taken'] for result in benchmark_results],\n",
    "        palette=\"viridis\",\n",
    "        ax=ax1\n",
    "    )\n",
    "    ax1.set_title('Response Time Comparison', fontsize=14, pad=20)\n",
    "    ax1.set_xlabel('Model', fontsize=12)\n",
    "    ax1.set_ylabel('Time (seconds)', fontsize=12)\n",
    "    \n",
    "    # Memory usage plot\n",
    "    sns.barplot(\n",
    "        x=[result['model'] for result in benchmark_results],\n",
    "        y=[result['metrics']['memory_used'] for result in benchmark_results],\n",
    "        palette=\"magma\",\n",
    "        ax=ax2\n",
    "    )\n",
    "    ax2.set_title('Memory Usage Comparison', fontsize=14, pad=20)\n",
    "    ax2.set_xlabel('Model', fontsize=12)\n",
    "    ax2.set_ylabel('Memory (MB)', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as img_stream:\n",
    "        plt.savefig(img_stream.name, format='png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        performance_path = img_stream.name\n",
    "\n",
    "    # Tokens per second comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x=[result['model'] for result in benchmark_results],\n",
    "        y=[result['metrics']['tokens_per_second'] for result in benchmark_results],\n",
    "        palette=\"rocket\"\n",
    "    )\n",
    "    plt.title('Processing Speed (Tokens/Second)', fontsize=14, pad=20)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Tokens per Second', fontsize=12)\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as img_stream:\n",
    "        plt.savefig(img_stream.name, format='png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        speed_path = img_stream.name\n",
    "\n",
    "    return performance_path, speed_path\n",
    "\n",
    "# [Previous imports remain the same until the create_pdf_report function]\n",
    "\n",
    "def create_enhanced_pdf_report(benchmark_results, performance_path, speed_path, filename=\"benchmark_results.pdf\"):\n",
    "    doc = SimpleDocTemplate(\n",
    "        filename,\n",
    "        pagesize=letter,\n",
    "        rightMargin=72,\n",
    "        leftMargin=72,\n",
    "        topMargin=72,\n",
    "        bottomMargin=72\n",
    "    )\n",
    "\n",
    "    # Enhanced styles\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30,\n",
    "        alignment=1,\n",
    "        textColor=HexColor('#2c3e50'),\n",
    "        fontName='Helvetica-Bold'\n",
    "    )\n",
    "    \n",
    "    section_heading = ParagraphStyle(\n",
    "        'SectionHeading',\n",
    "        parent=styles['Heading2'],\n",
    "        fontSize=18,\n",
    "        spaceBefore=20,\n",
    "        spaceAfter=12,\n",
    "        textColor=HexColor('#34495e'),\n",
    "        fontName='Helvetica-Bold',\n",
    "        borderPadding=10,\n",
    "        borderWidth=1,\n",
    "        borderColor=HexColor('#bdc3c7'),\n",
    "        borderRadius=8\n",
    "    )\n",
    "    \n",
    "    subsection_heading = ParagraphStyle(\n",
    "        'SubsectionHeading',\n",
    "        parent=styles['Heading3'],\n",
    "        fontSize=14,\n",
    "        spaceBefore=15,\n",
    "        spaceAfter=8,\n",
    "        textColor=HexColor('#2980b9'),\n",
    "        fontName='Helvetica-Bold'\n",
    "    )\n",
    "    \n",
    "    body_text = ParagraphStyle(\n",
    "        'BodyText',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=11,\n",
    "        spaceBefore=6,\n",
    "        spaceAfter=6,\n",
    "        leading=14,\n",
    "        alignment=0\n",
    "    )\n",
    "    \n",
    "    metric_text = ParagraphStyle(\n",
    "        'MetricText',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=10,\n",
    "        spaceBefore=4,\n",
    "        spaceAfter=4,\n",
    "        leftIndent=20,\n",
    "        textColor=HexColor('#444444')\n",
    "    )\n",
    "    \n",
    "    response_text = ParagraphStyle(\n",
    "        'ResponseText',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=10,\n",
    "        spaceBefore=6,\n",
    "        spaceAfter=6,\n",
    "        leftIndent=20,\n",
    "        rightIndent=20,\n",
    "        leading=12,\n",
    "        backColor=HexColor('#f7f9fc'),\n",
    "        borderPadding=10\n",
    "    )\n",
    "\n",
    "    content = []\n",
    "    \n",
    "    # Title Page\n",
    "    content.append(Paragraph(\"Model Benchmarking Report\", title_style))\n",
    "    content.append(Spacer(1, 0.5 * inch))\n",
    "    \n",
    "    # Date and Summary\n",
    "    content.append(Paragraph(f\"Report Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\", body_text))\n",
    "    content.append(Spacer(1, 0.3 * inch))\n",
    "\n",
    "    # Executive Summary\n",
    "    content.append(Paragraph(\"Executive Summary\", section_heading))\n",
    "    fastest_model = min(benchmark_results, key=lambda x: x['metrics']['time_taken'])\n",
    "    most_efficient_model = min(benchmark_results, key=lambda x: x['metrics']['memory_used'])\n",
    "    highest_throughput = max(benchmark_results, key=lambda x: x['metrics']['tokens_per_second'])\n",
    "    \n",
    "    summary_items = [\n",
    "        f\"• Fastest Response: {fastest_model['model']} ({fastest_model['metrics']['time_taken']:.2f} seconds)\",\n",
    "        f\"• Most Memory-Efficient: {most_efficient_model['model']} ({most_efficient_model['metrics']['memory_used']:.2f} MB)\",\n",
    "        f\"• Highest Throughput: {highest_throughput['model']} ({highest_throughput['metrics']['tokens_per_second']:.2f} tokens/second)\",\n",
    "        f\"• Total Models Tested: {len(benchmark_results)}\",\n",
    "    ]\n",
    "    \n",
    "    for item in summary_items:\n",
    "        content.append(Paragraph(item, body_text))\n",
    "    \n",
    "    content.append(Spacer(1, 0.3 * inch))\n",
    "\n",
    "    # Performance Visualizations\n",
    "    content.append(Paragraph(\"Performance Analysis\", section_heading))\n",
    "    content.append(Paragraph(\"Response Time and Memory Usage\", subsection_heading))\n",
    "    content.append(Image(performance_path, width=7*inch, height=7*inch))\n",
    "    content.append(Spacer(1, 0.2 * inch))\n",
    "    \n",
    "    content.append(Paragraph(\"Processing Speed Analysis\", subsection_heading))\n",
    "    content.append(Image(speed_path, width=7*inch, height=4*inch))\n",
    "    content.append(Spacer(1, 0.3 * inch))\n",
    "    \n",
    "    # Detailed Model Analysis\n",
    "    content.append(Paragraph(\"Detailed Model Analysis\", section_heading))\n",
    "    \n",
    "    for result in benchmark_results:\n",
    "        # Model Header\n",
    "        content.append(Paragraph(f\"Model: {result['model']}\", subsection_heading))\n",
    "        \n",
    "        # Performance Metrics\n",
    "        metrics = [\n",
    "            f\"• Response Time: {result['metrics']['time_taken']:.2f} seconds\",\n",
    "            f\"• Memory Usage: {result['metrics']['memory_used']:.2f} MB\",\n",
    "            f\"• Processing Speed: {result['metrics']['tokens_per_second']:.2f} tokens/second\"\n",
    "        ]\n",
    "        \n",
    "        content.append(Paragraph(\"Performance Metrics:\", body_text))\n",
    "        for metric in metrics:\n",
    "            content.append(Paragraph(metric, metric_text))\n",
    "            \n",
    "        # Model Response\n",
    "        content.append(Paragraph(\"Sample Response:\", body_text))\n",
    "        response_text_formatted = result['metrics']['result_text'].replace('\\n', '<br/>')\n",
    "        content.append(Paragraph(response_text_formatted, response_text))\n",
    "        \n",
    "        # Add comparison to average\n",
    "        avg_time = sum(r['metrics']['time_taken'] for r in benchmark_results) / len(benchmark_results)\n",
    "        avg_memory = sum(r['metrics']['memory_used'] for r in benchmark_results) / len(benchmark_results)\n",
    "        \n",
    "        performance_comparison = [\n",
    "            f\"• Time vs Average: {((result['metrics']['time_taken'] - avg_time) / avg_time * 100):.1f}% \" +\n",
    "            (\"faster\" if result['metrics']['time_taken'] < avg_time else \"slower\"),\n",
    "            f\"• Memory vs Average: {((result['metrics']['memory_used'] - avg_memory) / avg_memory * 100):.1f}% \" +\n",
    "            (\"less\" if result['metrics']['memory_used'] < avg_memory else \"more\")\n",
    "        ]\n",
    "        \n",
    "        content.append(Paragraph(\"Comparison to Average:\", body_text))\n",
    "        for comp in performance_comparison:\n",
    "            content.append(Paragraph(comp, metric_text))\n",
    "            \n",
    "        content.append(Spacer(1, 0.3 * inch))\n",
    "\n",
    "    # Build PDF\n",
    "    doc.build(content)\n",
    "\n",
    "    # Cleanup\n",
    "    os.remove(performance_path)\n",
    "    os.remove(speed_path)\n",
    "\n",
    "# [Rest of the code remains the same]\n",
    "\n",
    "def main():\n",
    "    # Initialize\n",
    "    combined_documents = load_and_process_documents()\n",
    "    db_combined = create_vector_store(combined_documents)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Answer the following question based only on the provided context.\n",
    "    Think step by step before providing a detailed answer.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {input}\"\"\")\n",
    "\n",
    "    # Extended model list\n",
    "    models = [\n",
    "        \"llama3.2\",\n",
    "        \"gemma2:2b\",\n",
    "        \"deepseek-r1:1.5b\",\n",
    "        \"tinyllama\"\n",
    "    ]\n",
    "    \n",
    "    benchmark_results = []\n",
    "\n",
    "    # Run benchmarks\n",
    "    print(\"\\nStarting benchmarking process...\")\n",
    "    for model in models:\n",
    "        print(f\"\\nBenchmarking {model}...\")\n",
    "        try:\n",
    "            metrics = benchmark_model(model, db_combined, prompt)\n",
    "            print(f\"✓ {model} benchmarked successfully\")\n",
    "            benchmark_results.append({\n",
    "                'model': model,\n",
    "                'metrics': metrics\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error benchmarking {model}: {str(e)}\")\n",
    "\n",
    "    # Generate report\n",
    "    performance_path, speed_path = generate_visualizations(benchmark_results)\n",
    "    create_pdf_report(benchmark_results, performance_path, speed_path)\n",
    "    print(\"\\n✓ PDF report generated successfully as 'benchmark_results.pdf'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 18:03:59,690 - INFO - Using documents path: c:\\Users\\infog\\Documents\\LinkedIn\\Benchmark\\documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting document processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 18:03:59,923 - INFO - Successfully loaded: bank details.pdf\n",
      "2025-01-31 18:03:59,973 - INFO - Successfully loaded: user data.pdf\n",
      "2025-01-31 18:03:59,974 - INFO - Processed 11 document chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Documents processed successfully\n",
      "\n",
      "Creating vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 18:04:18,561 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-01-31 18:04:18,574 - INFO - Vector store created with embedding model: llama2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vector store created successfully\n",
      "\n",
      "Starting benchmarking process...\n",
      "\n",
      "Benchmarking llama2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 18:04:19,964 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-01-31 18:04:22,473 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import ParagraphStyle, getSampleStyleSheet\n",
    "from reportlab.lib.colors import HexColor\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('benchmark.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_and_process_documents(docs_dir=\"documents\"):\n",
    "    \"\"\"\n",
    "    Load and preprocess PDF documents from the specified directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        docs_path = Path(docs_dir)\n",
    "        logging.info(f\"Using documents path: {docs_path.absolute()}\")\n",
    "        \n",
    "        if not docs_path.exists():\n",
    "            raise FileNotFoundError(f\"Documents directory not found: {docs_path}\")\n",
    "        \n",
    "        # Load documents\n",
    "        docs_combined = []\n",
    "        for pdf_file in docs_path.glob(\"*.pdf\"):\n",
    "            try:\n",
    "                loader = PyPDFLoader(str(pdf_file))\n",
    "                docs_combined.extend(loader.load())\n",
    "                logging.info(f\"Successfully loaded: {pdf_file.name}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading {pdf_file.name}: {str(e)}\")\n",
    "        \n",
    "        if not docs_combined:\n",
    "            raise ValueError(\"No documents were successfully loaded\")\n",
    "        \n",
    "        # Split documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "        split_docs = text_splitter.split_documents(docs_combined)\n",
    "        logging.info(f\"Processed {len(split_docs)} document chunks\")\n",
    "        \n",
    "        return split_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in document processing setup: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_vector_store(documents, embedding_model=\"llama2\"):\n",
    "    \"\"\"\n",
    "    Create a FAISS vector store from the processed documents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(model=embedding_model)\n",
    "        vector_store = FAISS.from_documents(documents, embeddings)\n",
    "        logging.info(f\"Vector store created with embedding model: {embedding_model}\")\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating vector store: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def benchmark_model(model_name, db_combined, prompt):\n",
    "    \"\"\"\n",
    "    Benchmark a model's performance across multiple test queries.\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=model_name)\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    retriever_combined = db_combined.as_retriever()\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"give me plan my study schedule within exam date\",\n",
    "        \"what are the key topics I need to focus on?\",\n",
    "        \"how should I organize my study materials?\",\n",
    "        \"what is the most effective way to prepare for the exam?\",\n",
    "        \"how can I track my study progress?\"\n",
    "    ]\n",
    "    \n",
    "    metrics = {\n",
    "        'responses': [],\n",
    "        'total_time': 0,\n",
    "        'total_memory': 0,\n",
    "        'total_tokens': 0\n",
    "    }\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    \n",
    "    for query in test_queries:\n",
    "        try:\n",
    "            # Memory benchmarking\n",
    "            memory_before = process.memory_info().rss\n",
    "            \n",
    "            # Response time benchmarking\n",
    "            start_time = time.time()\n",
    "            response = create_retrieval_chain(retriever_combined, document_chain).invoke({\"input\": query})\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Extract response text\n",
    "            if isinstance(response, dict):\n",
    "                response_text = response.get('answer', response.get('output', str(response)))\n",
    "            else:\n",
    "                response_text = str(response)\n",
    "                \n",
    "            # Calculate metrics for this query\n",
    "            query_time = end_time - start_time\n",
    "            query_memory = (process.memory_info().rss - memory_before) / (1024 * 1024)  # MB\n",
    "            query_tokens = len(str(response)) / query_time  # Approximate\n",
    "            \n",
    "            # Store detailed response data\n",
    "            metrics['responses'].append({\n",
    "                'query': query,\n",
    "                'response': response_text,\n",
    "                'time_taken': query_time,\n",
    "                'memory_used': query_memory,\n",
    "                'tokens_per_second': query_tokens\n",
    "            })\n",
    "            \n",
    "            # Update totals\n",
    "            metrics['total_time'] += query_time\n",
    "            metrics['total_memory'] += query_memory\n",
    "            metrics['total_tokens'] += query_tokens\n",
    "            \n",
    "            logging.info(f\"Successfully benchmarked {model_name} for query: {query[:50]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error benchmarking {model_name} for query '{query}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate averages\n",
    "    num_queries = len(metrics['responses'])\n",
    "    if num_queries > 0:\n",
    "        metrics['avg_time'] = metrics['total_time'] / num_queries\n",
    "        metrics['avg_memory'] = metrics['total_memory'] / num_queries\n",
    "        metrics['avg_tokens_per_second'] = metrics['total_tokens'] / num_queries\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def generate_visualizations(benchmark_results):\n",
    "    \"\"\"\n",
    "    Generate performance visualization plots.\n",
    "    \"\"\"\n",
    "    # Set style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "    # Time and Memory comparison\n",
    "    fig1, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "    \n",
    "    # Performance plot\n",
    "    sns.barplot(\n",
    "        x=[result['model'] for result in benchmark_results],\n",
    "        y=[result['metrics']['avg_time'] for result in benchmark_results],\n",
    "        palette=\"viridis\",\n",
    "        ax=ax1\n",
    "    )\n",
    "    ax1.set_title('Average Response Time Comparison', fontsize=14, pad=20)\n",
    "    ax1.set_xlabel('Model', fontsize=12)\n",
    "    ax1.set_ylabel('Time (seconds)', fontsize=12)\n",
    "    \n",
    "    # Memory usage plot\n",
    "    sns.barplot(\n",
    "        x=[result['model'] for result in benchmark_results],\n",
    "        y=[result['metrics']['avg_memory'] for result in benchmark_results],\n",
    "        palette=\"magma\",\n",
    "        ax=ax2\n",
    "    )\n",
    "    ax2.set_title('Average Memory Usage Comparison', fontsize=14, pad=20)\n",
    "    ax2.set_xlabel('Model', fontsize=12)\n",
    "    ax2.set_ylabel('Memory (MB)', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as img_stream:\n",
    "        plt.savefig(img_stream.name, format='png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        performance_path = img_stream.name\n",
    "\n",
    "    # Processing speed plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x=[result['model'] for result in benchmark_results],\n",
    "        y=[result['metrics']['avg_tokens_per_second'] for result in benchmark_results],\n",
    "        palette=\"rocket\"\n",
    "    )\n",
    "    plt.title('Average Processing Speed (Tokens/Second)', fontsize=14, pad=20)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Tokens per Second', fontsize=12)\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as img_stream:\n",
    "        plt.savefig(img_stream.name, format='png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        speed_path = img_stream.name\n",
    "\n",
    "    return performance_path, speed_path\n",
    "\n",
    "def create_enhanced_pdf_report(benchmark_results, performance_path, speed_path, filename=\"benchmark_results.pdf\"):\n",
    "    \"\"\"\n",
    "    Create a detailed PDF report of the benchmark results.\n",
    "    \"\"\"\n",
    "    doc = SimpleDocTemplate(\n",
    "        filename,\n",
    "        pagesize=letter,\n",
    "        rightMargin=72,\n",
    "        leftMargin=72,\n",
    "        topMargin=72,\n",
    "        bottomMargin=72\n",
    "    )\n",
    "\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    # Define custom styles\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30,\n",
    "        alignment=1,\n",
    "        textColor=HexColor('#2c3e50'),\n",
    "        fontName='Helvetica-Bold'\n",
    "    )\n",
    "    \n",
    "    section_heading = ParagraphStyle(\n",
    "        'SectionHeading',\n",
    "        parent=styles['Heading2'],\n",
    "        fontSize=18,\n",
    "        spaceBefore=20,\n",
    "        spaceAfter=12,\n",
    "        textColor=HexColor('#34495e'),\n",
    "        fontName='Helvetica-Bold',\n",
    "        borderPadding=10,\n",
    "        borderWidth=1,\n",
    "        borderColor=HexColor('#bdc3c7'),\n",
    "        borderRadius=8\n",
    "    )\n",
    "    \n",
    "    subsection_heading = ParagraphStyle(\n",
    "        'SubsectionHeading',\n",
    "        parent=styles['Heading3'],\n",
    "        fontSize=14,\n",
    "        spaceBefore=15,\n",
    "        spaceAfter=8,\n",
    "        textColor=HexColor('#2980b9'),\n",
    "        fontName='Helvetica-Bold'\n",
    "    )\n",
    "    \n",
    "    body_text = ParagraphStyle(\n",
    "        'BodyText',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=11,\n",
    "        spaceBefore=6,\n",
    "        spaceAfter=6,\n",
    "        leading=14\n",
    "    )\n",
    "    \n",
    "    metric_text = ParagraphStyle(\n",
    "        'MetricText',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=10,\n",
    "        spaceBefore=4,\n",
    "        spaceAfter=4,\n",
    "        leftIndent=20,\n",
    "        textColor=HexColor('#444444')\n",
    "    )\n",
    "    \n",
    "    response_text = ParagraphStyle(\n",
    "        'ResponseText',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=10,\n",
    "        spaceBefore=6,\n",
    "        spaceAfter=6,\n",
    "        leftIndent=20,\n",
    "        rightIndent=20,\n",
    "        leading=12,\n",
    "        backColor=HexColor('#f7f9fc'),\n",
    "        borderPadding=10,\n",
    "        borderWidth=1,\n",
    "        borderColor=HexColor('#e1e8ed'),\n",
    "        borderRadius=5\n",
    "    )\n",
    "\n",
    "    content = []\n",
    "    \n",
    "    # Title Page\n",
    "    content.append(Paragraph(\"LLM Model Benchmarking Report\", title_style))\n",
    "    content.append(Spacer(1, 0.5 * inch))\n",
    "    \n",
    "    # Date and Summary\n",
    "    content.append(Paragraph(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", body_text))\n",
    "    content.append(Spacer(1, 0.3 * inch))\n",
    "\n",
    "    # Executive Summary\n",
    "    content.append(Paragraph(\"Executive Summary\", section_heading))\n",
    "    fastest_model = min(benchmark_results, key=lambda x: x['metrics']['avg_time'])\n",
    "    most_efficient_model = min(benchmark_results, key=lambda x: x['metrics']['avg_memory'])\n",
    "    highest_throughput = max(benchmark_results, key=lambda x: x['metrics']['avg_tokens_per_second'])\n",
    "    \n",
    "    summary_items = [\n",
    "        f\"• Fastest Model: {fastest_model['model']} ({fastest_model['metrics']['avg_time']:.2f} seconds average)\",\n",
    "        f\"• Most Memory-Efficient: {most_efficient_model['model']} ({most_efficient_model['metrics']['avg_memory']:.2f} MB average)\",\n",
    "        f\"• Highest Throughput: {highest_throughput['model']} ({highest_throughput['metrics']['avg_tokens_per_second']:.2f} tokens/second)\",\n",
    "        f\"• Total Models Tested: {len(benchmark_results)}\",\n",
    "        f\"• Queries per Model: {len(benchmark_results[0]['metrics']['responses'])}\"\n",
    "    ]\n",
    "    \n",
    "    for item in summary_items:\n",
    "        content.append(Paragraph(item, body_text))\n",
    "    \n",
    "    content.append(Spacer(1, 0.3 * inch))\n",
    "\n",
    "    # Performance Visualizations\n",
    "    content.append(Paragraph(\"Performance Analysis\", section_heading))\n",
    "    content.append(Image(performance_path, width=7*inch, height=7*inch))\n",
    "    content.append(Spacer(1, 0.2 * inch))\n",
    "    content.append(Image(speed_path, width=7*inch, height=4*inch))\n",
    "    content.append(Spacer(1, 0.3 * inch))\n",
    "    \n",
    "    # Detailed Model Analysis\n",
    "    content.append(Paragraph(\"Detailed Model Analysis\", section_heading))\n",
    "    \n",
    "    for result in benchmark_results:\n",
    "        # Model Header\n",
    "        content.append(Paragraph(f\"Model: {result['model']}\", subsection_heading))\n",
    "        \n",
    "        # Overall Performance Metrics\n",
    "        content.append(Paragraph(\"Average Performance Metrics:\", body_text))\n",
    "        metrics = [\n",
    "            f\"• Average Response Time: {result['metrics']['avg_time']:.2f} seconds\",\n",
    "            f\"• Average Memory Usage: {result['metrics']['avg_memory']:.2f} MB\",\n",
    "            f\"• Average Processing Speed: {result['metrics']['avg_tokens_per_second']:.2f} tokens/second\"\n",
    "        ]\n",
    "        \n",
    "        for metric in metrics:\n",
    "            content.append(Paragraph(metric, metric_text))\n",
    "        \n",
    "        # Response Analysis\n",
    "        content.append(Paragraph(\"Sample Responses:\", subsection_heading))\n",
    "        \n",
    "        for response_data in result['metrics']['responses']:\n",
    "            # Query Info\n",
    "            content.append(Paragraph(f\"Query: {response_data['query']}\", body_text))\n",
    "            \n",
    "            # Response Metrics\n",
    "            response_metrics = [\n",
    "                f\"• Response Time: {response_data['time_taken']:.2f} seconds\",\n",
    "                f\"• Memory Used: {response_data['memory_used']:.2f} MB\",\n",
    "                f\"• Processing Speed: {response_data['tokens_per_second']:.2f} tokens/second\"\n",
    "            ]\n",
    "            \n",
    "            for metric in response_metrics:\n",
    "                content.append(Paragraph(metric, metric_text))\n",
    "            \n",
    "            # Format and display the response\n",
    "            formatted_response = response_data['response'].replace('\\n', '<br/>')\n",
    "            content.append(Paragraph(\"Response:\", body_text))\n",
    "            content.append(Paragraph(formatted_response, response_text))\n",
    "            content.append(Spacer(1, 0.2 * inch))\n",
    "        \n",
    "        content.append(Spacer(1, 0.3 * inch))\n",
    "\n",
    "    # Build PDF\n",
    "    doc.build(content)\n",
    "    \n",
    "    # Cleanup temporary image files\n",
    "    os.remove(performance_path)\n",
    "    os.remove(speed_path)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize\n",
    "        print(\"\\nStarting document processing...\")\n",
    "        combined_documents = load_and_process_documents()\n",
    "        print(\"✓ Documents processed successfully\")\n",
    "        \n",
    "        print(\"\\nCreating vector store...\")\n",
    "        db_combined = create_vector_store(combined_documents)\n",
    "        print(\"✓ Vector store created successfully\")\n",
    "        \n",
    "        # Define the prompt template\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Answer the following question based only on the provided context.\n",
    "        Think step by step before providing a detailed answer.\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "        Question: {input}\"\"\")\n",
    "\n",
    "        # Models to benchmark\n",
    "        models = [\n",
    "            \"llama2\", \n",
    "            \"mistral\", \n",
    "            \"neural-chat\",\n",
    "            \"openhermes\"\n",
    "        ]\n",
    "        \n",
    "        benchmark_results = []\n",
    "\n",
    "        # Run benchmarks\n",
    "        print(\"\\nStarting benchmarking process...\")\n",
    "        for model in models:\n",
    "            print(f\"\\nBenchmarking {model}...\")\n",
    "            try:\n",
    "                metrics = benchmark_model(model, db_combined, prompt)\n",
    "                print(f\"✓ {model} benchmarked successfully\")\n",
    "                benchmark_results.append({\n",
    "                    'model': model,\n",
    "                    'metrics': metrics\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error benchmarking {model}: {str(e)}\")\n",
    "                logging.error(f\"Error benchmarking {model}: {str(e)}\")\n",
    "\n",
    "        # Generate report\n",
    "        print(\"\\nGenerating visualizations and report...\")\n",
    "        performance_path, speed_path = generate_visualizations(benchmark_results)\n",
    "        create_enhanced_pdf_report(benchmark_results, performance_path, speed_path)\n",
    "        print(\"\\n✓ PDF report generated successfully as 'benchmark_results.pdf'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error during benchmark process: {str(e)}\")\n",
    "        logging.error(f\"Error during benchmark process: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
