{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import ParagraphStyle, getSampleStyleSheet\n",
    "from reportlab.lib.colors import HexColor\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
    "from reportlab.lib.units import inch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Processing PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('benchmark.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_and_process_documents(docs_dir=\"documents\"):\n",
    "    \"\"\"\n",
    "    Load and preprocess PDF documents from the specified directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        docs_path = Path(docs_dir)\n",
    "        logging.info(f\"Using documents path: {docs_path.absolute()}\")\n",
    "        \n",
    "        if not docs_path.exists():\n",
    "            raise FileNotFoundError(f\"Documents directory not found: {docs_path}\")\n",
    "        \n",
    "        # Load documents\n",
    "        docs_combined = []\n",
    "        for pdf_file in docs_path.glob(\"*.pdf\"):\n",
    "            try:\n",
    "                loader = PyPDFLoader(str(pdf_file))\n",
    "                docs_combined.extend(loader.load())\n",
    "                logging.info(f\"Successfully loaded: {pdf_file.name}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading {pdf_file.name}: {str(e)}\")\n",
    "        \n",
    "        if not docs_combined:\n",
    "            raise ValueError(\"No documents were successfully loaded\")\n",
    "        \n",
    "        # Split documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "        split_docs = text_splitter.split_documents(docs_combined)\n",
    "        logging.info(f\"Processed {len(split_docs)} document chunks\")\n",
    "        \n",
    "        return split_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in document processing setup: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Vector Store and Benchmarking Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vector_store(documents, embedding_model=\"llama2\"):\n",
    "    \"\"\"\n",
    "    Create a FAISS vector store from the processed documents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(model=embedding_model)\n",
    "        vector_store = FAISS.from_documents(documents, embeddings)\n",
    "        logging.info(f\"Vector store created with embedding model: {embedding_model}\")\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating vector store: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def benchmark_model(model_name, db_combined, prompt):\n",
    "    \"\"\"\n",
    "    Benchmark a model's performance across multiple test queries.\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=model_name)\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    retriever_combined = db_combined.as_retriever()\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"give me plan my study schedule within exam date\",\n",
    "        \"what are the key topics I need to focus on?\",\n",
    "        \"how should I organize my study materials?\",\n",
    "        \"what is the most effective way to prepare for the exam?\",\n",
    "        \"how can I track my study progress?\"\n",
    "    ]\n",
    "    \n",
    "    metrics = {\n",
    "        'responses': [],\n",
    "        'total_time': 0,\n",
    "        'total_memory': 0,\n",
    "        'total_tokens': 0,\n",
    "        'avg_time': 0,  # Initialize average metrics\n",
    "        'avg_memory': 0,\n",
    "        'avg_tokens_per_second': 0\n",
    "    }\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    \n",
    "    for query in test_queries:\n",
    "        try:\n",
    "            # Memory benchmarking\n",
    "            memory_before = process.memory_info().rss\n",
    "            \n",
    "            # Response time benchmarking\n",
    "            start_time = time.time()\n",
    "            response = create_retrieval_chain(retriever_combined, document_chain).invoke({\"input\": query})\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Extract response text\n",
    "            if isinstance(response, dict):\n",
    "                response_text = response.get('answer', response.get('output', str(response)))\n",
    "            else:\n",
    "                response_text = str(response)\n",
    "                \n",
    "            # Calculate metrics for this query\n",
    "            query_time = end_time - start_time\n",
    "            query_memory = (process.memory_info().rss - memory_before) / (1024 * 1024)  # MB\n",
    "            query_tokens = len(str(response)) / query_time  # Approximate\n",
    "            \n",
    "            # Store detailed response data\n",
    "            metrics['responses'].append({\n",
    "                'query': query,\n",
    "                'response': response_text,\n",
    "                'time_taken': query_time,\n",
    "                'memory_used': query_memory,\n",
    "                'tokens_per_second': query_tokens\n",
    "            })\n",
    "            \n",
    "            # Update totals\n",
    "            metrics['total_time'] += query_time\n",
    "            metrics['total_memory'] += query_memory\n",
    "            metrics['total_tokens'] += query_tokens\n",
    "            \n",
    "            logging.info(f\"Successfully benchmarked {model_name} for query: {query[:50]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error benchmarking {model_name} for query '{query}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate averages\n",
    "    num_queries = len(metrics['responses'])\n",
    "    if num_queries > 0:\n",
    "        metrics['avg_time'] = metrics['total_time'] / num_queries\n",
    "        metrics['avg_memory'] = metrics['total_memory'] / num_queries\n",
    "        metrics['avg_tokens_per_second'] = metrics['total_tokens'] / num_queries\n",
    "    else:\n",
    "        # Handle the case where no queries were successful\n",
    "        logging.warning(f\"No successful queries for model {model_name}\")\n",
    "        metrics['avg_time'] = 0\n",
    "        metrics['avg_memory'] = 0\n",
    "        metrics['avg_tokens_per_second'] = 0\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_visualizations(benchmark_results):\n",
    "    \"\"\"\n",
    "    Generate performance visualization plots.\n",
    "    \"\"\"\n",
    "    # Set style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "    # Time and Memory comparison\n",
    "    fig1, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "    \n",
    "    # Performance plot\n",
    "    sns.barplot(\n",
    "        x=[result['model'] for result in benchmark_results],\n",
    "        y=[result['metrics']['avg_time'] for result in benchmark_results],\n",
    "        palette=\"rocket\",\n",
    "        ax=ax1\n",
    "    )\n",
    "    ax1.set_title('Average Response Time Comparison', fontsize=14, pad=20)\n",
    "    ax1.set_xlabel('Model', fontsize=12)\n",
    "    ax1.set_ylabel('Time (seconds)', fontsize=12)\n",
    "    \n",
    "    # Memory usage plot\n",
    "    sns.barplot(\n",
    "        x=[result['model'] for result in benchmark_results],\n",
    "        y=[result['metrics']['avg_memory'] for result in benchmark_results],\n",
    "        palette=\"magma\",\n",
    "        ax=ax2\n",
    "    )\n",
    "    ax2.set_title('Average Memory Usage Comparison', fontsize=14, pad=20)\n",
    "    ax2.set_xlabel('Model', fontsize=12)\n",
    "    ax2.set_ylabel('Memory (MB)', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as img_stream:\n",
    "        plt.savefig(img_stream.name, format='png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        performance_path = img_stream.name\n",
    "\n",
    "    # Processing speed plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x=[result['model'] for result in benchmark_results],\n",
    "        y=[result['metrics']['avg_tokens_per_second'] for result in benchmark_results],\n",
    "        palette=\"rocket\"\n",
    "    )\n",
    "    plt.title('Average Processing Speed (Tokens/Second)', fontsize=14, pad=20)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Tokens per Second', fontsize=12)\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as img_stream:\n",
    "        plt.savefig(img_stream.name, format='png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        speed_path = img_stream.name\n",
    "\n",
    "    return performance_path, speed_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Enhanced PDF Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_enhanced_pdf_report(benchmark_results, performance_path, speed_path, filename=\"benchmark_results.pdf\"):\n",
    "    \"\"\"\n",
    "    Create a detailed PDF report of the benchmark results.\n",
    "    \"\"\"\n",
    "    doc = SimpleDocTemplate(\n",
    "        filename,\n",
    "        pagesize=letter,\n",
    "        rightMargin=72,\n",
    "        leftMargin=72,\n",
    "        topMargin=72,\n",
    "        bottomMargin=72\n",
    "    )\n",
    "\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    # Define custom styles\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30,\n",
    "        alignment=1,\n",
    "        textColor=HexColor('#2c3e50'),\n",
    "        fontName='Helvetica-Bold'\n",
    "    )\n",
    "    \n",
    "    section_heading = ParagraphStyle(\n",
    "        'SectionHeading',\n",
    "        parent=styles['Heading2'],\n",
    "        fontSize=18,\n",
    "        spaceBefore=20,\n",
    "        spaceAfter=12,\n",
    "        textColor=HexColor('#34495e'),\n",
    "        fontName='Helvetica-Bold',\n",
    "        borderPadding=10,\n",
    "        borderWidth=1,\n",
    "        borderColor=HexColor('#bdc3c7'),\n",
    "        borderRadius=8\n",
    "    )\n",
    "    \n",
    "    subsection_heading = ParagraphStyle(\n",
    "        'SubsectionHeading',\n",
    "        parent=styles['Heading3'],\n",
    "        fontSize=14,\n",
    "        spaceBefore=15,\n",
    "        spaceAfter=8,\n",
    "        textColor=HexColor('#2980b9'),\n",
    "        fontName='Helvetica-Bold'\n",
    "    )\n",
    "    \n",
    "    body_text = ParagraphStyle(\n",
    "        'BodyText',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=11,\n",
    "        spaceBefore=6,\n",
    "        spaceAfter=6,\n",
    "        leading=14\n",
    "    )\n",
    "    \n",
    "    metric_text = ParagraphStyle(\n",
    "        'MetricText',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=10,\n",
    "        spaceBefore=4,\n",
    "        spaceAfter=4,\n",
    "        leftIndent=20,\n",
    "        textColor=HexColor('#444444')\n",
    "    )\n",
    "    \n",
    "    response_text = ParagraphStyle(\n",
    "        'ResponseText',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=10,\n",
    "        spaceBefore=6,\n",
    "        spaceAfter=6,\n",
    "        leftIndent=20,\n",
    "        rightIndent=20,\n",
    "        leading=12,\n",
    "        backColor=HexColor('#f7f9fc'),\n",
    "        borderPadding=10,\n",
    "        borderWidth=1,\n",
    "        borderColor=HexColor('#e1e8ed'),\n",
    "        borderRadius=5\n",
    "    )\n",
    "\n",
    "    content = []\n",
    "    \n",
    "    # Title Page\n",
    "    content.append(Paragraph(\"LLM Model Benchmarking Report\", title_style))\n",
    "    content.append(Spacer(1, 0.5 * inch))\n",
    "    \n",
    "    # Date and Summary\n",
    "    content.append(Paragraph(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", body_text))\n",
    "    content.append(Spacer(1, 0.3 * inch))\n",
    "\n",
    "    # Executive Summary\n",
    "    content.append(Paragraph(\"Executive Summary\", section_heading))\n",
    "    fastest_model = min(benchmark_results, key=lambda x: x['metrics']['avg_time'])\n",
    "    most_efficient_model = min(benchmark_results, key=lambda x: x['metrics']['avg_memory'])\n",
    "    highest_throughput = max(benchmark_results, key=lambda x: x['metrics']['avg_tokens_per_second'])\n",
    "    \n",
    "    summary_items = [\n",
    "        f\"• Fastest Model: {fastest_model['model']} ({fastest_model['metrics']['avg_time']:.2f} seconds average)\",\n",
    "        f\"• Most Memory-Efficient: {most_efficient_model['model']} ({most_efficient_model['metrics']['avg_memory']:.2f} MB average)\",\n",
    "        f\"• Highest Throughput: {highest_throughput['model']} ({highest_throughput['metrics']['avg_tokens_per_second']:.2f} tokens/second)\",\n",
    "        f\"• Total Models Tested: {len(benchmark_results)}\",\n",
    "        f\"• Queries per Model: {len(benchmark_results[0]['metrics']['responses'])}\"\n",
    "    ]\n",
    "    \n",
    "    for item in summary_items:\n",
    "        content.append(Paragraph(item, body_text))\n",
    "    \n",
    "    content.append(Spacer(1, 0.3 * inch))\n",
    "\n",
    "    # Performance Visualizations\n",
    "    content.append(Paragraph(\"Performance Analysis\", section_heading))\n",
    "    content.append(Image(performance_path, width=7*inch, height=7*inch))\n",
    "    content.append(Spacer(1, 0.2 * inch))\n",
    "    content.append(Image(speed_path, width=7*inch, height=4*inch))\n",
    "    content.append(Spacer(1, 0.3 * inch))\n",
    "    \n",
    "    # Detailed Model Analysis\n",
    "    content.append(Paragraph(\"Detailed Model Analysis\", section_heading))\n",
    "    \n",
    "    for result in benchmark_results:\n",
    "        # Model Header\n",
    "        content.append(Paragraph(f\"Model: {result['model']}\", subsection_heading))\n",
    "        \n",
    "        # Overall Performance Metrics\n",
    "        content.append(Paragraph(\"Average Performance Metrics:\", body_text))\n",
    "        metrics = [\n",
    "            f\"• Average Response Time: {result['metrics']['avg_time']:.2f} seconds\",\n",
    "            f\"• Average Memory Usage: {result['metrics']['avg_memory']:.2f} MB\",\n",
    "            f\"• Average Processing Speed: {result['metrics']['avg_tokens_per_second']:.2f} tokens/second\"\n",
    "        ]\n",
    "        \n",
    "        for metric in metrics:\n",
    "            content.append(Paragraph(metric, metric_text))\n",
    "        \n",
    "        # Response Analysis\n",
    "        content.append(Paragraph(\"Sample Responses:\", subsection_heading))\n",
    "        \n",
    "        for response_data in result['metrics']['responses']:\n",
    "            # Query Info\n",
    "            content.append(Paragraph(f\"Query: {response_data['query']}\", body_text))\n",
    "            \n",
    "            # Response Metrics\n",
    "            response_metrics = [\n",
    "                f\"• Response Time: {response_data['time_taken']:.2f} seconds\",\n",
    "                f\"• Memory Used: {response_data['memory_used']:.2f} MB\",\n",
    "                f\"• Processing Speed: {response_data['tokens_per_second']:.2f} tokens/second\"\n",
    "            ]\n",
    "            \n",
    "            for metric in response_metrics:\n",
    "                content.append(Paragraph(metric, metric_text))\n",
    "            \n",
    "            # Format and display the response\n",
    "            formatted_response = response_data['response'].replace('\\n', '<br/>')\n",
    "            content.append(Paragraph(\"Response:\", body_text))\n",
    "            content.append(Paragraph(formatted_response, response_text))\n",
    "            content.append(Spacer(1, 0.2 * inch))\n",
    "        \n",
    "        content.append(Spacer(1, 0.3 * inch))\n",
    "\n",
    "    # Build PDF\n",
    "    doc.build(content)\n",
    "    \n",
    "    # Cleanup temporary image files\n",
    "    os.remove(performance_path)\n",
    "    os.remove(speed_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 17:17:42,280 - INFO - Using documents path: c:\\Users\\infog\\Documents\\LinkedIn\\Benchmark\\documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting document processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 17:17:42,573 - INFO - Successfully loaded: bank details.pdf\n",
      "2025-02-03 17:17:42,624 - INFO - Successfully loaded: user data.pdf\n",
      "2025-02-03 17:17:42,626 - INFO - Processed 11 document chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Documents processed successfully\n",
      "\n",
      "Creating vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 17:18:00,965 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:18:00,984 - INFO - Loading faiss with AVX512 support.\n",
      "2025-02-03 17:18:00,984 - INFO - Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "2025-02-03 17:18:00,985 - INFO - Loading faiss with AVX2 support.\n",
      "2025-02-03 17:18:01,250 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-02-03 17:18:01,260 - INFO - Vector store created with embedding model: llama2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vector store created successfully\n",
      "\n",
      "Starting benchmarking process...\n",
      "\n",
      "Benchmarking llama3.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 17:18:02,538 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:18:06,375 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:18:18,379 - INFO - Successfully benchmarked llama3.2 for query: give me plan my study schedule within exam date...\n",
      "2025-02-03 17:18:23,947 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:18:27,468 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:18:33,182 - INFO - Successfully benchmarked llama3.2 for query: what are the key topics I need to focus on?...\n",
      "2025-02-03 17:18:37,242 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:18:40,193 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:19:03,374 - INFO - Successfully benchmarked llama3.2 for query: how should I organize my study materials?...\n",
      "2025-02-03 17:19:10,796 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:19:15,144 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:19:42,552 - INFO - Successfully benchmarked llama3.2 for query: what is the most effective way to prepare for the ...\n",
      "2025-02-03 17:19:46,918 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:19:51,031 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:20:03,946 - INFO - Successfully benchmarked llama3.2 for query: how can I track my study progress?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ llama3.2 benchmarked successfully\n",
      "\n",
      "Benchmarking gemma2:2b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 17:20:09,168 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:20:13,616 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:20:30,630 - INFO - Successfully benchmarked gemma2:2b for query: give me plan my study schedule within exam date...\n",
      "2025-02-03 17:20:34,058 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:20:36,546 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:20:41,555 - INFO - Successfully benchmarked gemma2:2b for query: what are the key topics I need to focus on?...\n",
      "2025-02-03 17:20:44,783 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:20:47,243 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:20:56,007 - INFO - Successfully benchmarked gemma2:2b for query: how should I organize my study materials?...\n",
      "2025-02-03 17:20:59,458 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:21:01,886 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:21:11,352 - INFO - Successfully benchmarked gemma2:2b for query: what is the most effective way to prepare for the ...\n",
      "2025-02-03 17:21:15,578 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:21:18,657 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:21:25,742 - INFO - Successfully benchmarked gemma2:2b for query: how can I track my study progress?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ gemma2:2b benchmarked successfully\n",
      "\n",
      "Benchmarking deepseek-r1:1.5b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 17:21:32,249 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:21:35,066 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:21:53,162 - INFO - Successfully benchmarked deepseek-r1:1.5b for query: give me plan my study schedule within exam date...\n",
      "2025-02-03 17:21:57,491 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:21:59,626 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:22:09,974 - INFO - Successfully benchmarked deepseek-r1:1.5b for query: what are the key topics I need to focus on?...\n",
      "2025-02-03 17:22:15,336 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:22:18,225 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:22:28,826 - INFO - Successfully benchmarked deepseek-r1:1.5b for query: how should I organize my study materials?...\n",
      "2025-02-03 17:22:32,568 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:22:35,073 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:22:47,039 - INFO - Successfully benchmarked deepseek-r1:1.5b for query: what is the most effective way to prepare for the ...\n",
      "2025-02-03 17:22:50,528 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:22:52,580 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:23:02,320 - INFO - Successfully benchmarked deepseek-r1:1.5b for query: how can I track my study progress?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ deepseek-r1:1.5b benchmarked successfully\n",
      "\n",
      "Benchmarking smollm2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 17:23:06,756 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:23:09,940 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:23:19,118 - INFO - Successfully benchmarked smollm2 for query: give me plan my study schedule within exam date...\n",
      "2025-02-03 17:23:24,236 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:23:26,839 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:23:30,594 - INFO - Successfully benchmarked smollm2 for query: what are the key topics I need to focus on?...\n",
      "2025-02-03 17:23:35,638 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:23:38,280 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:23:44,535 - INFO - Successfully benchmarked smollm2 for query: how should I organize my study materials?...\n",
      "2025-02-03 17:23:49,532 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:23:52,094 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:23:58,902 - INFO - Successfully benchmarked smollm2 for query: what is the most effective way to prepare for the ...\n",
      "2025-02-03 17:24:02,303 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:24:04,574 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:24:11,899 - INFO - Successfully benchmarked smollm2 for query: how can I track my study progress?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ smollm2 benchmarked successfully\n",
      "\n",
      "Benchmarking granite3.1-dense:2b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 17:24:16,120 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:24:18,952 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:24:26,557 - INFO - Successfully benchmarked granite3.1-dense:2b for query: give me plan my study schedule within exam date...\n",
      "2025-02-03 17:24:30,200 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:24:32,475 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:24:37,576 - INFO - Successfully benchmarked granite3.1-dense:2b for query: what are the key topics I need to focus on?...\n",
      "2025-02-03 17:24:41,005 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:24:43,224 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:24:51,126 - INFO - Successfully benchmarked granite3.1-dense:2b for query: how should I organize my study materials?...\n",
      "2025-02-03 17:24:54,845 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:24:57,369 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:25:07,257 - INFO - Successfully benchmarked granite3.1-dense:2b for query: what is the most effective way to prepare for the ...\n",
      "2025-02-03 17:25:10,600 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:25:12,778 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:25:19,457 - INFO - Successfully benchmarked granite3.1-dense:2b for query: how can I track my study progress?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ granite3.1-dense:2b benchmarked successfully\n",
      "\n",
      "Benchmarking qwen2:1.5b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 17:25:23,668 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:25:26,230 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:25:31,902 - INFO - Successfully benchmarked qwen2:1.5b for query: give me plan my study schedule within exam date...\n",
      "2025-02-03 17:25:40,196 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:25:42,303 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:25:44,744 - INFO - Successfully benchmarked qwen2:1.5b for query: what are the key topics I need to focus on?...\n",
      "2025-02-03 17:25:52,940 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:25:55,020 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:25:58,977 - INFO - Successfully benchmarked qwen2:1.5b for query: how should I organize my study materials?...\n",
      "2025-02-03 17:26:07,370 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:26:09,142 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:26:10,305 - INFO - Successfully benchmarked qwen2:1.5b for query: what is the most effective way to prepare for the ...\n",
      "2025-02-03 17:26:18,344 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:26:20,167 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 17:26:23,659 - INFO - Successfully benchmarked qwen2:1.5b for query: how can I track my study progress?...\n",
      "C:\\Users\\infog\\AppData\\Local\\Temp\\ipykernel_16868\\958426899.py:13: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ qwen2:1.5b benchmarked successfully\n",
      "\n",
      "Generating visualizations and report...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\infog\\AppData\\Local\\Temp\\ipykernel_16868\\958426899.py:24: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n",
      "C:\\Users\\infog\\AppData\\Local\\Temp\\ipykernel_16868\\958426899.py:43: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ PDF report generated successfully as 'benchmark_results.pdf'\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Initialize\n",
    "        print(\"\\nStarting document processing...\")\n",
    "        combined_documents = load_and_process_documents()\n",
    "        print(\"✓ Documents processed successfully\")\n",
    "        \n",
    "        print(\"\\nCreating vector store...\")\n",
    "        db_combined = create_vector_store(combined_documents)\n",
    "        print(\"✓ Vector store created successfully\")\n",
    "        \n",
    "        # Define the prompt template\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Answer the following question based only on the provided context.\n",
    "        Think step by step before providing a detailed answer.\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "        Question: {input}\"\"\")\n",
    "\n",
    "        # Models to benchmark\n",
    "        models = [\n",
    "            \"llama3.2\", \n",
    "            \"gemma2:2b\", \n",
    "            \"deepseek-r1:1.5b\",\n",
    "            \"smollm2\",\n",
    "            \"granite3.1-dense:2b\",\n",
    "            \"qwen2:1.5b\",\n",
    "        ]\n",
    "        \n",
    "        benchmark_results = []\n",
    "\n",
    "        # Run benchmarks\n",
    "        print(\"\\nStarting benchmarking process...\")\n",
    "        for model in models:\n",
    "            print(f\"\\nBenchmarking {model}...\")\n",
    "            try:\n",
    "                metrics = benchmark_model(model, db_combined, prompt)\n",
    "                print(f\"✓ {model} benchmarked successfully\")\n",
    "                benchmark_results.append({\n",
    "                    'model': model,\n",
    "                    'metrics': metrics\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error benchmarking {model}: {str(e)}\")\n",
    "                logging.error(f\"Error benchmarking {model}: {str(e)}\")\n",
    "\n",
    "        # Generate report\n",
    "        print(\"\\nGenerating visualizations and report...\")\n",
    "        performance_path, speed_path = generate_visualizations(benchmark_results)\n",
    "        create_enhanced_pdf_report(benchmark_results, performance_path, speed_path)\n",
    "        print(\"\\n✓ PDF report generated successfully as 'benchmark_results.pdf'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error during benchmark process: {str(e)}\")\n",
    "        logging.error(f\"Error during benchmark process: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
